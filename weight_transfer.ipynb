{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "from transformers import FlaxWhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForConditionalGeneration were not initialized from the model checkpoint at openai/whisper-small and are newly initialized: ['proj_out.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the processor and model\n",
    "model_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/model_all4'\n",
    "pt_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "dtype = jnp.float32\n",
    "jax_model, params = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    model_path, dtype=dtype, _do_init=False)\n",
    "pt_model.config = jax_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes = jax.tree_map(lambda x:x.shape,params)\n",
    "#shapes['model']['encoder']['conv1']#['layer_norm']#['layers']['0']\n",
    "#pt_model.get_encoder().conv1.weight.shape#.layer_norm.weight.shape,pt_model.get_decoder().layer_norm.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'kernel':'weight',\\\n",
    "           'lm_head':'proj_out',\\\n",
    "           'embedding':'weight',\\\n",
    "           'scale':'weight'}\n",
    "\n",
    "def set_pytorch_weights(pytorch_model, jax_dict):\n",
    "    \"\"\"\n",
    "    Recursively set weights from the JAX dictionary to the PyTorch model.\n",
    "    \"\"\"\n",
    "    for key, value in jax_dict.items():\n",
    "        # Build the full key based on the current traversal\n",
    "        key = mapping[key] if key in mapping else key\n",
    "        \n",
    "        # If the value is another dictionary, we recurse deeper\n",
    "        if isinstance(value, dict):\n",
    "            set_pytorch_weights(getattr(pytorch_model,key), value)\n",
    "        else:\n",
    "            # Convert JAX value to PyTorch tensor\n",
    "            pytorch_tensor = torch.tensor(np.asarray(value),dtype=torch.float32)\n",
    "            if isinstance(pytorch_model,(torch.nn.Linear,torch.nn.Conv1d)):\n",
    "                pytorch_tensor = pytorch_tensor.T\n",
    "            pytorch_tensor = torch.nn.Parameter(pytorch_tensor)\n",
    "            setattr(pytorch_model, key, pytorch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40073/566656060.py:21: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343904639/work/aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  pytorch_tensor = pytorch_tensor.T\n"
     ]
    }
   ],
   "source": [
    "#%debug\n",
    "set_pytorch_weights(pt_model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check output matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "IsTPU = False\n",
    "batch_size = 64\n",
    "dtype = jnp.float16\n",
    "speech_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/dlsprint/validation_files/'\n",
    "data_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/dlsprint/validation.csv'\n",
    "add_audio2 = '/home/zhenlan/Desktop/Projects/Bengali ASR/RESPIN/dev/'\n",
    "num_workers = 8\n",
    "output_path = ''\n",
    "\n",
    "#from whisper_jax import FlaxWhisperForConditionalGeneration\n",
    "from transformers import FlaxWhisperForConditionalGeneration\n",
    "from functions import *\n",
    "from functools import partial\n",
    "import optax\n",
    "import evaluate\n",
    "from jax import random\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n"
     ]
    }
   ],
   "source": [
    "pad_to_multiple_of = 1\n",
    "max_length_gen = 24\n",
    "epochs = 1\n",
    "verbose = 5\n",
    "learning_rate=4e-4\n",
    "clip = 1e-2\n",
    "# tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", language=\"bn\", task=\"transcribe\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n",
    "tokenizer.bos_token = tokenizer.bos_token_id = None\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "text = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text',names=['code'])\n",
    "df[['code','sentence']] = df[\"code\"].str.split(\" \", n=1, expand=True)\n",
    "df.sentence = df.sentence.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = AudioDataset(text,speech_path,lambda x:x.path,\\\n",
    "                       None,orig_sr=32000, target_sr=16000)\n",
    "train_loader_CE = DataLoader(dataset1, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n",
    "                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n",
    "                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=True,IsTPU=IsTPU,batch_size=batch_size))\n",
    "train_loader_WER = DataLoader(dataset1, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n",
    "                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n",
    "                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=False,IsTPU=IsTPU,batch_size=batch_size))\n",
    "\n",
    "dataset2 = AudioDataset(df,add_audio2,\\\n",
    "                        lambda x:x.code.split('_')[-1] + '.wav',orig_sr=16000, target_sr=16000)\n",
    "train_loader_CE2 = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n",
    "                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n",
    "                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=True,IsTPU=IsTPU,batch_size=batch_size))\n",
    "train_loader_WER2 = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n",
    "                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n",
    "                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=False,IsTPU=IsTPU,batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,input_ids,attention_mask = next(iter(train_loader_CE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX\n",
    "out_jax = jax_model(audio,input_ids,decoder_attention_mask=attention_mask,params=params,train=False).logits\n",
    "# pytorch\n",
    "pt_model.eval()\n",
    "with torch.no_grad():\n",
    "    out_pt = pt_model(torch.tensor(audio,dtype=torch.float32),\\\n",
    "                      decoder_input_ids=torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                      decoder_attention_mask=torch.tensor(attention_mask,dtype=torch.long)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.772960193566416e-07"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_abs(x,y,mask=None):\n",
    "    if mask is None:\n",
    "        return np.mean(np.abs(np.array(x)-np.array(y)))\n",
    "    else:\n",
    "        return np.sum(np.abs(np.array(x)-np.array(y))*mask[...,None])/np.sum(mask)/51865\n",
    "mean_abs(out_jax,out_pt,attention_mask)/out_pt.abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_jax = jax_model.encode(input_features=jnp.array(audio,dtype=jnp.float32),params=params)\n",
    "# pt_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     encoder_pt = pt_model.get_encoder()(input_features=torch.tensor(audio,dtype=torch.float32))\n",
    "# mean_abs(encoder_jax.last_hidden_state,encoder_pt.last_hidden_state)\n",
    "# outputs_jax = jax_model.decode(input_ids, encoder_jax,params=params)\n",
    "# encoder_pt.last_hidden_state.shape\n",
    "# pt_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs_pt = pt_model.get_decoder()(torch.tensor(input_ids,dtype=torch.long),encoder_hidden_states=encoder_pt.last_hidden_state)\n",
    "# mean_abs(outputs_jax.logits,encoder_pt.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in pt_model.named_parameters():\n",
    "#     jax_name_list = name.split('.')\n",
    "#     tmp_param = params\n",
    "#     while jax_name_list:\n",
    "#         tmp_name = jax_name_list.pop(0)\n",
    "#         if tmp_name=='proj_out':\n",
    "#             tmp_name = 'lm_head'\n",
    "#         elif tmp_name=='weight':\n",
    "#             if 'embed' in name:\n",
    "#                 tmp_name = 'embedding'\n",
    "#             elif 'norm' in name:\n",
    "#                 tmp_name = 'scale'\n",
    "#             else:\n",
    "#                 tmp_name = 'kernel'\n",
    "#         tmp_param = tmp_param[tmp_name]\n",
    "#     pt_,jax_ = np.array(param.detach()),np.array(tmp_param)\n",
    "#     if 'conv' in name or 'fc' in name or 'proj' in name:\n",
    "#         jax_ = jax_.T\n",
    "#     assert np.allclose(pt_,jax_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
