{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:45:20.390377Z","iopub.status.busy":"2023-07-30T13:45:20.390053Z","iopub.status.idle":"2023-07-30T13:45:24.039550Z","shell.execute_reply":"2023-07-30T13:45:24.038543Z","shell.execute_reply.started":"2023-07-30T13:45:20.390351Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#from whisper_jax import FlaxWhisperForConditionalGeneration\n","# from transformers import WhisperForConditionalGeneration\n","import torch\n","from functions import *\n","from functools import partial\n","import math\n","import evaluate\n","from transformers import Wav2Vec2ProcessorWithLM,Wav2Vec2Processor\n","from pyctcdecode import build_ctcdecoder\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","IsTPU = False\n","batch_size = 16\n","device = 'cuda'\n","dtype = torch.float32\n","speech_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/dlsprint/validation_files/'\n","data_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/dlsprint/validation.csv'\n","add_audio2 = '/home/zhenlan/Desktop/Projects/Bengali ASR/RESPIN/dev/'\n","model_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/IndicFT'\n","speech_path0 = 'data/train_mp3s/'\n","data_path0 = 'data/train.csv'\n","num_workers = 8\n","output_path = ''"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:46:20.634072Z","iopub.status.busy":"2023-07-30T13:46:20.633641Z","iopub.status.idle":"2023-07-30T13:46:24.324136Z","shell.execute_reply":"2023-07-30T13:46:24.322657Z","shell.execute_reply.started":"2023-07-30T13:46:20.634037Z"},"trusted":true},"outputs":[],"source":["pad_to_multiple_of = 1\n","max_length_gen = 24\n","\n","processor = Wav2Vec2ProcessorWithLM.from_pretrained(\"/home/zhenlan/Desktop/Projects/Bengali ASR/arijitx-wav2vec2-xls-r-300m-bengali\")\n","# processor = Wav2Vec2ProcessorWithLM.from_pretrained(\"/home/zhenlan/Desktop/Projects/Bengali ASR/YellowKing_processor\")\n","# processor = Wav2Vec2Processor.from_pretrained(model_path)\n","tokenizer = processor.tokenizer\n","feature_extractor = processor.feature_extractor\n","text = pd.read_csv(data_path)\n","text0 = pd.read_csv(data_path0)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# vocab_dict = processor.tokenizer.get_vocab()\n","# sorted_vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n","# decoder = build_ctcdecoder(\n","#     list(sorted_vocab_dict.keys()),\n","#     kenlm_model_path='/home/zhenlan/Desktop/Projects/Bengali ASR/arijitx-wav2vec2-xls-r-300m-bengali/language_model/5gram.bin',\n","#     # unigrams = '/home/zhenlan/Desktop/Projects/Bengali ASR/arijitx-wav2vec2-xls-r-300m-bengali/language_model/unigrams.txt',\n","#     # alpha=0.5,  # tuned on a val set\n","#     # beta=1.0,  # tuned on a val set\n","# )\n","# processor_with_lm = Wav2Vec2ProcessorWithLM(\n","#     feature_extractor=processor.feature_extractor,\n","#     tokenizer=processor.tokenizer,\n","#     decoder=decoder\n","# )"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df = pd.read_csv('text',names=['code'])\n","df[['code','sentence']] = df[\"code\"].str.split(\" \", n=1, expand=True)\n","df.sentence = df.sentence.str.strip()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:46:32.166568Z","iopub.status.busy":"2023-07-30T13:46:32.165544Z","iopub.status.idle":"2023-07-30T13:46:32.174103Z","shell.execute_reply":"2023-07-30T13:46:32.172823Z","shell.execute_reply.started":"2023-07-30T13:46:32.166526Z"},"trusted":true},"outputs":[],"source":["dataset0 = AudioDataset(text0,speech_path0,lambda x:x.id+'.mp3',\\\n","                        orig_sr=32000, target_sr=16000)\n","train_loader_CE0 = DataLoader(dataset0, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn_pt_wav2vec,tokenizer=tokenizer,feature_extractor=feature_extractor))\n","train_loader_WER0 = DataLoader(dataset0, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn_pt_wav2vec,tokenizer=tokenizer,feature_extractor=feature_extractor,IsTrain=False))\n","\n","dataset1 = AudioDataset(text,speech_path,lambda x:x.path,\\\n","                       None,orig_sr=32000, target_sr=16000)\n","train_loader_CE = DataLoader(dataset1, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn_pt_wav2vec,tokenizer=tokenizer,feature_extractor=feature_extractor))\n","train_loader_WER = DataLoader(dataset1, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn_pt_wav2vec,tokenizer=tokenizer,feature_extractor=feature_extractor,IsTrain=False))\n","\n","dataset2 = AudioDataset(df,add_audio2,\\\n","                        lambda x:x.code.split('_')[-1] + '.wav',orig_sr=16000, target_sr=16000)\n","train_loader_CE2 = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn_pt_wav2vec,tokenizer=tokenizer,feature_extractor=feature_extractor))\n","train_loader_WER2 = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn_pt_wav2vec,tokenizer=tokenizer,feature_extractor=feature_extractor,IsTrain=False))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:46:33.722832Z","iopub.status.busy":"2023-07-30T13:46:33.722475Z","iopub.status.idle":"2023-07-30T13:46:51.296694Z","shell.execute_reply":"2023-07-30T13:46:51.295376Z","shell.execute_reply.started":"2023-07-30T13:46:33.722802Z"},"trusted":true},"outputs":[],"source":["from transformers import Wav2Vec2ForCTC\n","# model = Wav2Vec2ForCTC.from_pretrained(\"Sameen53/training_45k\").to('cuda')\n","# model = Wav2Vec2ForCTC.from_pretrained(\"/home/zhenlan/Desktop/Projects/Bengali ASR/YellowKing_model\").to('cuda')\n","model = torch.load('/home/zhenlan/Desktop/Projects/Bengali ASR/wav2vec_CTC/wav2vec.pth').to('cuda')\n","# model = Wav2Vec2ForCTC.from_pretrained(model_path).to('cuda')\n","model.eval();"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from transformers import pipeline\n","my_asrLM = pipeline(\"automatic-speech-recognition\", \\\n","                    model=model ,\\\n","                    feature_extractor =processor.feature_extractor, \\\n","                    tokenizer= processor.tokenizer,decoder=processor.decoder ,device=0)\n","# audio,attention_mask,txt = next(iter(train_loader_WER0))\n","# out = my_asrLM(list(audio))\n","metric = evaluate.load(\"wer\")\n","def metric_one_step(audio,attention_mask,txt,beam_width=100):\n","    transcriptions = [item['text'] for item in my_asrLM(list(audio))]\n","    wer = metric.compute(predictions=transcriptions, references=txt)\n","    return wer"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# metric = evaluate.load(\"wer\")\n","# def metric_one_step(audio,attention_mask,txt,beam_width=100):\n","#     audio,attention_mask = torch.tensor(audio,device=device),\\\n","#                                  torch.tensor(attention_mask,device=device)\n","#     with torch.no_grad():\n","#         logits = model(audio,attention_mask=attention_mask).logits.cpu().numpy()\n","    \n","#     transcriptions = [processor_with_lm.decode(logit,beam_width=beam_width).text for logit in logits]\n","#     wer = metric.compute(predictions=transcriptions, references=txt)\n","#     return wer\n","\n","def eval_one_step(audio,input_ids,attention_mask):\n","    audio,input_ids,attention_mask = torch.tensor(audio,device=device),\\\n","                                 torch.tensor(input_ids,device=device),\\\n","                                 torch.tensor(attention_mask,device=device)\n","    with torch.no_grad():\n","        loss = model(audio,attention_mask=attention_mask,labels=input_ids).loss\n","        return loss"]},{"cell_type":"markdown","metadata":{},"source":["DATASET0"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import logging\n","logging.basicConfig(filename='performance/FTCTC0827_performance.log', filemode='w', level=logging.INFO,\n","                    format='%(message)s', datefmt='%Y-%m-%d %H:%M:%S')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 10.2 s, sys: 1.27 s, total: 11.5 s\n","Wall time: 12.2 s\n"]}],"source":["%%time\n","# # model_all2\n","test_loss = 0\n","for j,(audio,input_ids,attention_mask) in enumerate(train_loader_CE0):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = eval_one_step(audio,input_ids,attention_mask)\n","    if math.isinf(l.item()) or math.isnan(l.item()):\n","        test_loss += 5.0 # CTC loss overflow\n","    else:\n","        test_loss += l.item()\n","    if j>60:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"CE loss on data0: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# %%time\n","# model_all2\n","test_loss = 0\n","for j,(audio,attention_mask,txt) in enumerate(train_loader_WER0):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = metric_one_step(audio,attention_mask,txt)\n","    test_loss += l\n","    if j>60:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"WER loss on data0: {test_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["DATASET1"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 10.3 s, sys: 681 ms, total: 10.9 s\n","Wall time: 11.7 s\n"]}],"source":["%%time\n","# # model_all2\n","test_loss = 0\n","for j,(audio,input_ids,attention_mask) in enumerate(train_loader_CE):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = eval_one_step(audio,input_ids,attention_mask)\n","    if math.isinf(l.item()) or math.isnan(l.item()):\n","        test_loss += 5.0 # CTC loss overflow\n","    else:\n","        test_loss += l.item()\n","    if j>60:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"CE loss on data1: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 3min 17s, sys: 1.23 s, total: 3min 18s\n","Wall time: 39.2 s\n"]}],"source":["%%time\n","# model_all2\n","test_loss = 0\n","for j,(audio,attention_mask,txt) in enumerate(train_loader_WER):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = metric_one_step(audio,attention_mask,txt)\n","    test_loss += l\n","    if j>60:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"WER loss on data1: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["DATASET2"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 20.8 s, sys: 1.16 s, total: 22 s\n","Wall time: 23.1 s\n"]}],"source":["%%time\n","# # model_all2\n","test_loss = 0\n","for j,(audio,input_ids,attention_mask) in enumerate(train_loader_CE2):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = eval_one_step(audio,input_ids,attention_mask)\n","    if math.isinf(l.item()) or math.isnan(l.item()):\n","        test_loss += 5.0 # CTC loss overflow\n","    else:\n","        test_loss += l.item()\n","    if j>60:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"CE loss on data2: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 3min 30s, sys: 1.55 s, total: 3min 32s\n","Wall time: 55.9 s\n"]}],"source":["%%time\n","# model_all2\n","test_loss = 0\n","for j,(audio,attention_mask,txt) in enumerate(train_loader_WER2):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = metric_one_step(audio,attention_mask,txt)\n","    test_loss += l\n","    if j>60:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"WER loss on data2: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
