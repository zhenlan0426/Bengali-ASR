{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,Wav2Vec2Processor\n",
    "from transformers import WhisperForConditionalGeneration,Wav2Vec2ForCTC\n",
    "\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from functions import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import BaseModelOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two models and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5\")\n",
    "# use whisper as encoder\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained('/home/zhenlan/Desktop/Projects/Bengali ASR/model_all3')\n",
    "model.encoder = whisperEncoderWhead(whisper_model.get_encoder(),768,768)\n",
    "torch.save(model, '/home/zhenlan/Desktop/Projects/Bengali ASR/T5Model_whisper/t5.pth')\n",
    "# use wav2vec\n",
    "# wav2vec = Wav2Vec2ForCTC.from_pretrained(\"Sameen53/training_45k\")\n",
    "# model.encoder = whisperEncoderWhead(wav2vec.wav2vec2,1024,768)\n",
    "# torch.save(model, '/home/zhenlan/Desktop/Projects/Bengali ASR/T5Model_wav2vec/t5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.modeling_outputs import BaseModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class whisperEncoderWhead(torch.nn.Module):\n",
    "    def __init__(self, whisper) -> None:\n",
    "        super().__init__()\n",
    "        self.convert = torch.nn.Linear(768,768)\n",
    "        self.whisper = whisper\n",
    "\n",
    "    def forward(self,speech,*args,**kwargs):\n",
    "        out = self.whisper(speech)[0]\n",
    "        out = self.convert(out)\n",
    "        return BaseModelOutput(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/zhenlan/Desktop/Projects/Bengali ASR/T5Model/t5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.whisper.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_input = torch.rand(4, 80, 16)\n",
    "txt_input = torch.randint(0,128,(4,16))\n",
    "attention_mask = torch.rand((4,16))>0.3\n",
    "# out = model(input_ids=speech_input,labels=txt_input)\n",
    "logits = model(input_ids=speech_input,decoder_input_ids=txt_input).logits\n",
    "loss = torch.sum(torch.nn.CrossEntropyLoss(reduction='none')(logits[:,0:-1].reshape(-1,32128),\\\n",
    "                                                              txt_input[:,1:].reshape(-1))*attention_mask[:,1:].reshape(-1))/torch.sum(attention_mask[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/zhenlan/Desktop/Projects/Bengali ASR/T5Model/generation_config.json', 'r') as file:\n",
    "    generation_config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_dict(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = model.encoder(speech_input)\n",
    "out = model.generate(encoder_outputs=encoder_outputs,generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
