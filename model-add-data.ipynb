{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "FLATFORM = 'gcp'\n",
    "if FLATFORM=='kaggle':\n",
    "    IsTPU = True\n",
    "    dtype = jnp.bfloat16\n",
    "    num_workers = 16\n",
    "    batch_size = 12 * 8\n",
    "#     try:\n",
    "#         import transformers\n",
    "#         speech_path = 'bengaliai-speech/train_mp3s/'\n",
    "#         data_path = 'bengaliai-speech/train.csv'\n",
    "#         output_path = ''\n",
    "#     except:\n",
    "    !pip install librosa --quiet  --upgrade\n",
    "    !pip install git+https://github.com/zhenlan0426/transformers.git --quiet  --upgrade\n",
    "    #!pip install evaluate --quiet  --upgrade\n",
    "    !pip install sentencepiece --quiet  --upgrade\n",
    "    #!pip install jiwer --quiet  --upgrade\n",
    "    !pip install audiomentations --quiet  --upgrade\n",
    "    speech_path = '/kaggle/input/bengaliai-speech/train_mp3s/'\n",
    "    data_path = '/kaggle/input/bengaliai-speech/train.csv'\n",
    "    output_path = '/kaggle/working/'\n",
    "    background = '/kaggle/input/esc-50/audio'\n",
    "    model_path = '/kaggle/input/model-add-data-all/model_all'\n",
    "    add_data_path = '/kaggle/input/dlsprint/train.csv'\n",
    "    add_audio = '/kaggle/input/dlsprint/train_files/'\n",
    "elif FLATFORM=='gcp':\n",
    "    #!pip install tensorrt --upgrade\n",
    "    #%pip install git+https://github.com/zhenlan0426/transformers.git  --upgrade\n",
    "    IsTPU = True\n",
    "    dtype = jnp.bfloat16\n",
    "    num_workers = 16\n",
    "    batch_size = 12 * 8\n",
    "    speech_path = 'data/train_mp3s/'\n",
    "    data_path = 'data/train.csv'\n",
    "    output_path = ''\n",
    "    background = 'background/audio/'\n",
    "    model_path = 'model_all'\n",
    "    add_data_path = 'dlsprint/train.csv'\n",
    "    add_audio = 'dlsprint/train_files/'\n",
    "    add_data_path2 = 'text'\n",
    "    add_audio2 = '/mnt/disks/persist/RESPIN/'\n",
    "else:\n",
    "    IsTPU = False\n",
    "    batch_size = 32\n",
    "    dtype = jnp.float16\n",
    "    speech_path = 'data/train_mp3s/'\n",
    "    data_path = 'data/train.csv'\n",
    "    num_workers = 0\n",
    "    output_path = ''\n",
    "    background = \"/home/zhenlan/Desktop/Projects/Bengali ASR/ESC-50-master/audio\"\n",
    "    add_data_path2 = '/home/zhenlan/Desktop/Projects/Bengali ASR/text'\n",
    "    add_audio2 = '/home/zhenlan/Desktop/Projects/Bengali ASR/RESPIN/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlanwang0426/.venv311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#from whisper_jax import FlaxWhisperForConditionalGeneration\n",
    "from transformers import FlaxWhisperForConditionalGeneration\n",
    "from functions import *\n",
    "from functools import partial\n",
    "import optax\n",
    "from jax import random\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n"
     ]
    }
   ],
   "source": [
    "pad_to_multiple_of = 1\n",
    "max_length_gen = 48\n",
    "epochs = 1\n",
    "verbose = 5\n",
    "\n",
    "\n",
    "# tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", language=\"bn\", task=\"transcribe\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n",
    "tokenizer.bos_token = tokenizer.bos_token_id = None\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "text = pd.read_csv(data_path)\n",
    "add_data = pd.read_csv(add_data_path)\n",
    "# data source 2\n",
    "df = pd.read_csv(add_data_path2,names=['code'])\n",
    "df[['code','sentence']] = df[\"code\"].str.split(\" \", n=1, expand=True)\n",
    "df.sentence = df.sentence.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://github.com/karoldvl/ESC-50/archive/master.zip\"\n",
    "from audiomentations import (\n",
    "    AddBackgroundNoise,\n",
    "    AddGaussianNoise,\n",
    "    Compose,\n",
    "    Gain,\n",
    "    GainTransition,\n",
    "    OneOf,\n",
    "    PitchShift,\n",
    "    PolarityInversion,\n",
    "    TimeStretch,\n",
    "    )\n",
    "\n",
    "# define augmentation\n",
    "augmentation = Compose(\n",
    "    [\n",
    "        TimeStretch(min_rate=0.9, max_rate=1.1, p=1, leave_length_unchanged=False),\n",
    "        Gain(min_gain_db=-6, max_gain_db=6, p=1),\n",
    "        GainTransition(min_gain_db=-6, max_gain_db=6),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=1),\n",
    "        AddBackgroundNoise(sounds_path=background, min_snr_in_db=1.0, max_snr_in_db=5.0, noise_transform=PolarityInversion(), p=1.0),\n",
    "        AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=1),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = AudioDataset(text,speech_path,lambda x:x.id+'.mp3',\\\n",
    "                       augmentation,orig_sr=32000, target_sr=16000)\n",
    "dataset2 = AudioDataset(add_data,add_audio,\\\n",
    "                        lambda x:x.path,augmentation,orig_sr=48000, target_sr=16000)\n",
    "dataset3 = AudioDataset(df,add_audio2,\\\n",
    "                        lambda x:x.code.split('_')[-1] + '.wav',augmentation,orig_sr=16000, target_sr=16000)\n",
    "dataset = Add2Data(dataset1,dataset2)\n",
    "dataset = Add2Data(dataset,dataset3)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, \\\n",
    "                        collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n",
    "                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=True,IsTPU=IsTPU,batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio,input_ids,attention_mask = next(iter(train_loader))\n",
    "# audio.shape,input_ids.shape\n",
    "# audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1126498304 bytes == 0x9b2f0000 @  0x7f8bdd6cd680 0x7f8bdd6ee824 0x5e06a0 0x6381fd 0x6785d1 0x5e72b4 0x53724e 0x67775e 0x6258ab 0x635d05 0x58b1da 0x6da3bf 0x6da647 0x6dc2b3 0x58e8d5 0x70b63c 0x63f58e 0x58dffe 0x70b63c 0x63f58e 0x58dffe 0x70b63c 0x70b8d7 0x6251f7 0x635d05 0x58b1da 0x5644be 0x6316c3 0x633f31 0x58ed6d 0x70b63c\n"
     ]
    }
   ],
   "source": [
    "# load the processor and model\n",
    "model, params = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    model_path, dtype=dtype, _do_init=False)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.bos_token_id = None\n",
    "model.config.suppress_tokens = None\n",
    "model.config.decoder_start_token_id = None\n",
    "model.generation_config.decoder_start_token_id = [50258, 50302, 50359, 50363]# '<|startoftranscript|><|bn|><|transcribe|><|notimestamps|>\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\"\"\"A list of pairs of integers which indicates a mapping from generation indices to token indices \n",
    "that will be forced before sampling. For example, [[0, 123]] means the first generated token \n",
    "will always be a token of index 123.\"\"\"\n",
    "model.generation_config.suppress_tokens = None\n",
    "model.generation_config.begin_suppress_tokens = None\n",
    "model.generation_config.bos_token_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params['lm_head'] = {'kernel':params['model']['decoder']['embed_tokens']['embedding'].T,\\\n",
    "#                      'bias'  :jnp.zeros(model.config.vocab_size,params['model']['decoder']['embed_tokens']['embedding'].dtype)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "opt = optax.adamw(learning_rate=4e-5,\n",
    "                  b1=0.9,\n",
    "                  b2=0.98,\n",
    "                  eps=1e-6,\n",
    "                  weight_decay=1e-1,)#1e-1\n",
    "opt = optax.chain(optax.clip_by_global_norm(4e-3),opt)\n",
    "#opt_states = opt.init(embedding)\n",
    "# opt_states = opt.init(params)\n",
    "filehandler = open(output_path+\"opt_states\",\"rb\")\n",
    "opt_states = pickle.load(filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(jax.pmap,axis_name='data',in_axes=(None,None,0,0,0,None),out_axes=(None,None,None))\n",
    "# def train_one_step_embed(embedding,params,audio,input_ids,attention_mask,opt_states):\n",
    "#     def loss_fn(embedding,params,audio,input_ids,attention_mask):\n",
    "#         params['model']['decoder']['embed_tokens']['embedding'] = embedding\n",
    "#         out = model(audio,input_ids,decoder_attention_mask=attention_mask,params=params,train=True).logits # (B, L, d)\n",
    "#         return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(out[:,3:-1], input_ids[:,4:])*attention_mask[:,4:])\n",
    "#     grad_fn = jax.value_and_grad(loss_fn,has_aux=False)\n",
    "#     out = grad_fn(embedding,params,audio,input_ids,attention_mask)\n",
    "#     l,grads = jax.lax.pmean(out,'data')\n",
    "#     updates, opt_states = opt.update(grads, opt_states,params=embedding)\n",
    "#     embedding = optax.apply_updates(embedding, updates)\n",
    "#     return embedding,opt_states,l\n",
    "\n",
    "@partial(jax.pmap,axis_name='data')\n",
    "def train_one_step(params,audio,input_ids,attention_mask,opt_states):\n",
    "    def loss_fn(params,audio,input_ids,attention_mask):\n",
    "        out = model(audio,input_ids,decoder_attention_mask=attention_mask,params=params,train=True).logits # (B, L, d)\n",
    "        return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(out[:,3:-1], input_ids[:,4:])*attention_mask[:,4:])\n",
    "    grad_fn = jax.value_and_grad(loss_fn,has_aux=False)\n",
    "    out = grad_fn(params,audio,input_ids,attention_mask)\n",
    "    l,grads = jax.lax.pmean(out,'data')\n",
    "    updates, opt_states = opt.update(grads, opt_states,params=params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params,opt_states,l\n",
    "\n",
    "# @jax.jit\n",
    "# def train_one_step(params,audio,input_ids,attention_mask,opt_states):\n",
    "#     def loss_fn(params,audio,input_ids,attention_mask):\n",
    "#         out = model(audio,input_ids,decoder_attention_mask=attention_mask,params=params,train=True).logits # (B, L, d)\n",
    "#         return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(out[:,3:-1], input_ids[:,4:])*attention_mask[:,4:])\n",
    "#     grad_fn = jax.value_and_grad(loss_fn,has_aux=False)\n",
    "#     l,grads = grad_fn(params,audio,input_ids,attention_mask)\n",
    "#     updates, opt_states = opt.update(grads, opt_states,params=params)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     return params,opt_states,l\n",
    "\n",
    "# @jax.jit\n",
    "# def train_one_step_embed(embedding,params,audio,input_ids,attention_mask,opt_states):\n",
    "#     def loss_fn(embedding,params,audio,input_ids,attention_mask):\n",
    "#         params['model']['decoder']['embed_tokens']['embedding'] = embedding\n",
    "#         out = model(audio,input_ids,decoder_attention_mask=attention_mask,params=params,train=True).logits # (B, L, d)\n",
    "#         return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(out[:,3:-1], multiple hosts are needed, and some finesse is required to manage them.input_ids[:,4:])*attention_mask[:,4:])\n",
    "#     grad_fn = jax.value_and_grad(loss_fn,has_aux=False)\n",
    "#     l,grads = grad_fn(embedding,params,audio,input_ids,attention_mask)\n",
    "#     updates, opt_states = opt.update(grads, opt_states,params=embedding)\n",
    "#     embedding = optax.apply_updates(embedding, updates)\n",
    "#     return embedding,opt_states,l\n",
    "\n",
    "# @jax.jit\n",
    "# def eval_one_step(params,audio,input_ids,attention_mask):\n",
    "#     out = model(audio,input_ids,decoder_attention_mask=attention_mask,params=params,train=False).logits # (B, L, d)\n",
    "#     return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(out[:,3:-1], input_ids[:,4:])*attention_mask[:,4:])\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# def generate(params,audio):\n",
    "#     return model.generate(audio,params=params,max_length=max_length_gen)\n",
    "\n",
    "# metric = evaluate.load(\"wer\")\n",
    "# def metric_one_step(params,audio,txt):\n",
    "#     generated_ids = model.generate(audio,params=params,max_length=max_length_gen, num_beams=1, do_sample=False).sequences\n",
    "#     transcriptions = tokenizer.batch_decode(generated_ids.tolist(), skip_special_tokens=True)\n",
    "#     wer = metric.compute(predictions=transcriptions, references=txt)\n",
    "#     return wer\n",
    "\n",
    "# def batch_generate(loader):\n",
    "#     pass\n",
    "#     #transcriptions = [txt + \"|\" for txt in transcriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:5, loss: 1.733\n",
      "iterations:10, loss: 1.502\n",
      "iterations:15, loss: 1.552\n",
      "iterations:20, loss: 1.644\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='training.log', filemode='w', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "devices = jax.devices()\n",
    "replicated_params = jax.device_put_replicated(params, devices)\n",
    "replicated_opt_states = jax.device_put_replicated(opt_states, devices)\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    # train\n",
    "    train_loss = 0\n",
    "    for j,(audio,input_ids,attention_mask) in enumerate(train_loader):\n",
    "#        audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n",
    "#         embedding,opt_states,l = train_one_step_embed(embedding,params,audio,input_ids,attention_mask,opt_states)\n",
    "        replicated_params,replicated_opt_states,l = train_one_step(replicated_params,audio,input_ids,attention_mask,replicated_opt_states)\n",
    "        train_loss += l[0].item()\n",
    "        if j>0 and j%verbose == 0:\n",
    "            train_loss /= verbose\n",
    "            logging.info(f\"iterations:{j}, loss: {train_loss:.3f}\")\n",
    "            train_loss = 0\n",
    "            if j%25 ==0:\n",
    "                model.save_pretrained(output_path+'model_all',jax.tree_map(lambda x: x[0], replicated_params))\n",
    "                filehandler = open(output_path+\"opt_states\",\"wb\")\n",
    "                pickle.dump(jax.tree_map(lambda x: x[0], replicated_opt_states),filehandler)\n",
    "                filehandler.close()\n",
    "                \n",
    "            if (time.time() - start)/60/60 > 24: # 4 hours\n",
    "                break\n",
    "        # # eval\n",
    "        # if j%verbose == 0:\n",
    "        #     train_loss /= verbose    \n",
    "        #     eval_metric = 0\n",
    "        #     params['model']['decoder']['embed_tokens']['embedding'] = embedding\n",
    "        #     for k,(audio,input_ids,attention_mask) in enumerate(test_loader):\n",
    "        #         audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n",
    "        #         eval_l = eval_one_step(params,audio,input_ids,attention_mask)\n",
    "        #         eval_metric += eval_l.item()\n",
    "            \n",
    "        #     eval_metric /= k\n",
    "        #     print(f\"iterations:{j}, loss: {train_loss:.3f}, wer: {eval_metric:.3f}\")\n",
    "        #     train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_path+'model_all',jax.tree_map(lambda x: x[0], replicated_params))\n",
    "filehandler = open(output_path+\"opt_states\",\"wb\")\n",
    "pickle.dump(jax.tree_map(lambda x: x[0], replicated_opt_states),filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
