{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:45:20.390377Z","iopub.status.busy":"2023-07-30T13:45:20.390053Z","iopub.status.idle":"2023-07-30T13:45:24.039550Z","shell.execute_reply":"2023-07-30T13:45:24.038543Z","shell.execute_reply.started":"2023-07-30T13:45:20.390351Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#from whisper_jax import FlaxWhisperForConditionalGeneration\n","from transformers import WhisperForConditionalGeneration\n","import torch\n","from functions import *\n","from functools import partial\n","\n","import evaluate\n","from transformers import AutoTokenizer\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","IsTPU = False\n","batch_size = 16\n","device = 'cuda'\n","dtype = torch.float32\n","speech_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/dlsprint/validation_files/'\n","data_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/dlsprint/validation.csv'\n","add_audio2 = '/home/zhenlan/Desktop/Projects/Bengali ASR/RESPIN/dev/'\n","model_path = '/home/zhenlan/Desktop/Projects/Bengali ASR/model_all3'\n","speech_path0 = 'data/train_mp3s/'\n","data_path0 = 'data/train.csv'\n","num_workers = 8\n","output_path = ''\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:46:20.634072Z","iopub.status.busy":"2023-07-30T13:46:20.633641Z","iopub.status.idle":"2023-07-30T13:46:24.324136Z","shell.execute_reply":"2023-07-30T13:46:24.322657Z","shell.execute_reply.started":"2023-07-30T13:46:20.634037Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n"]}],"source":["pad_to_multiple_of = 1\n","max_length_gen = 24\n","epochs = 1\n","verbose = 5\n","learning_rate=4e-4\n","clip = 1e-2\n","# tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", language=\"bn\", task=\"transcribe\")\n","#tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n","tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n","tokenizer.bos_token = tokenizer.bos_token_id = None\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")\n","text = pd.read_csv(data_path)\n","text0 = pd.read_csv(data_path0)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = pd.read_csv('text',names=['code'])\n","df[['code','sentence']] = df[\"code\"].str.split(\" \", n=1, expand=True)\n","df.sentence = df.sentence.str.strip()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:46:32.166568Z","iopub.status.busy":"2023-07-30T13:46:32.165544Z","iopub.status.idle":"2023-07-30T13:46:32.174103Z","shell.execute_reply":"2023-07-30T13:46:32.172823Z","shell.execute_reply.started":"2023-07-30T13:46:32.166526Z"},"trusted":true},"outputs":[],"source":["dataset0 = AudioDataset(text0,speech_path0,lambda x:x.id+'.mp3',\\\n","                        orig_sr=32000, target_sr=16000)\n","train_loader_CE0 = DataLoader(dataset0, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n","                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=True,IsTPU=IsTPU,batch_size=batch_size))\n","train_loader_WER0 = DataLoader(dataset0, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n","                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=False,IsTPU=IsTPU,batch_size=batch_size))\n","\n","dataset1 = AudioDataset(text,speech_path,lambda x:x.path,\\\n","                       None,orig_sr=32000, target_sr=16000)\n","train_loader_CE = DataLoader(dataset1, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n","                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=True,IsTPU=IsTPU,batch_size=batch_size))\n","train_loader_WER = DataLoader(dataset1, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n","                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=False,IsTPU=IsTPU,batch_size=batch_size))\n","\n","dataset2 = AudioDataset(df,add_audio2,\\\n","                        lambda x:x.code.split('_')[-1] + '.wav',orig_sr=16000, target_sr=16000)\n","train_loader_CE2 = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n","                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=True,IsTPU=IsTPU,batch_size=batch_size))\n","train_loader_WER2 = DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=num_workers, \\\n","                         collate_fn=partial(collate_fn,tokenizer=tokenizer,feature_extractor=feature_extractor,\\\n","                                           pad_to_multiple_of=pad_to_multiple_of,IsTrain=False,IsTPU=IsTPU,batch_size=batch_size))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T13:46:33.722832Z","iopub.status.busy":"2023-07-30T13:46:33.722475Z","iopub.status.idle":"2023-07-30T13:46:51.296694Z","shell.execute_reply":"2023-07-30T13:46:51.295376Z","shell.execute_reply.started":"2023-07-30T13:46:33.722802Z"},"trusted":true},"outputs":[],"source":["model = WhisperForConditionalGeneration.from_pretrained(model_path)\n","model.eval()\n","model.config.forced_decoder_ids = None\n","model.config.bos_token_id = None\n","model.config.suppress_tokens = None\n","model.config.decoder_start_token_id = None\n","model.generation_config.decoder_start_token_id = [50258, 50302, 50359, 50363]# '<|startoftranscript|><|bn|><|transcribe|><|notimestamps|>\n","model.generation_config.forced_decoder_ids = None\n","\"\"\"A list of pairs of integers which indicates a mapping from generation indices to token indices \n","that will be forced before sampling. For example, [[0, 123]] means the first generated token \n","will always be a token of index 123.\"\"\"\n","model.generation_config.suppress_tokens = None\n","model.generation_config.begin_suppress_tokens = None\n","model.generation_config.bos_token_id = None\n","model = model.to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["metric = evaluate.load(\"wer\")\n","def metric_one_step(audio,txt):\n","    with torch.no_grad():\n","        # generated_ids = model.generate(torch.tensor(audio,device=device),max_length=max_length_gen, num_beams=1, do_sample=False)\n","        generated_ids = model.generate(torch.tensor(audio,device=device),\\\n","                               max_length=max_length_gen,num_beams=5,\\\n","                               early_stopping=True, no_repeat_ngram_size=2,\\\n","                               min_new_tokens=2, eos_token_id=50257,do_sample=False)\n","    transcriptions = tokenizer.batch_decode(generated_ids.tolist(), skip_special_tokens=True)\n","    wer = metric.compute(predictions=transcriptions, references=txt)\n","    return wer\n","\n","def eval_one_step(audio,input_ids,attention_mask):\n","    audio,input_ids,attention_mask = torch.tensor(audio,dtype=dtype,device=device),\\\n","                                 torch.tensor(input_ids,dtype=torch.long,device=device),\\\n","                                 torch.tensor(attention_mask,dtype=torch.long,device=device)\n","    with torch.no_grad():\n","        logits = model(audio,decoder_input_ids=input_ids,decoder_attention_mask=attention_mask).logits\n","        l = torch.mean(torch.nn.CrossEntropyLoss(reduction='none')(logits[:,3:-1].reshape(-1,51865),input_ids[:,4:].reshape(-1))*attention_mask[:,4:].reshape(-1))\n","        return l"]},{"cell_type":"markdown","metadata":{},"source":["DATASET0"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import logging\n","logging.basicConfig(filename=model_path+'/'+'performance.log', filemode='w', level=logging.INFO,\n","                    format='%(message)s', datefmt='%Y-%m-%d %H:%M:%S')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 1.23 s, sys: 356 ms, total: 1.59 s\n","Wall time: 2.31 s\n"]}],"source":["%%time\n","# # model_all2\n","test_loss = 0\n","for j,(audio,input_ids,attention_mask) in enumerate(train_loader_CE0):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = eval_one_step(audio,input_ids,attention_mask)\n","    test_loss += l.item()\n","    if j>20:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"CE loss on data0: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 6.68 s, sys: 211 ms, total: 6.89 s\n","Wall time: 7.51 s\n"]}],"source":["%%time\n","# model_all2\n","test_loss = 0\n","for j,(audio,txt) in enumerate(train_loader_WER0):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = metric_one_step(audio,txt)\n","    test_loss += l\n","    if j>20:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"WER loss on data0: {test_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["DATASET1"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 1.19 s, sys: 215 ms, total: 1.4 s\n","Wall time: 2.28 s\n"]}],"source":["%%time\n","# # model_all2\n","test_loss = 0\n","for j,(audio,input_ids,attention_mask) in enumerate(train_loader_CE):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = eval_one_step(audio,input_ids,attention_mask)\n","    test_loss += l.item()\n","    if j>20:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"CE loss on data1: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 7.21 s, sys: 184 ms, total: 7.39 s\n","Wall time: 8.25 s\n"]}],"source":["%%time\n","# model_all2\n","test_loss = 0\n","for j,(audio,txt) in enumerate(train_loader_WER):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = metric_one_step(audio,txt)\n","    test_loss += l\n","    if j>20:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"WER loss on data1: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["DATASET2"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 2.22 s, sys: 562 ms, total: 2.78 s\n","Wall time: 3.62 s\n"]}],"source":["%%time\n","# # model_all2\n","test_loss = 0\n","for j,(audio,input_ids,attention_mask) in enumerate(train_loader_CE2):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = eval_one_step(audio,input_ids,attention_mask)\n","    test_loss += l.item()\n","    if j>20:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"CE loss on data2: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 10.4 s, sys: 559 ms, total: 11 s\n","Wall time: 11.8 s\n"]}],"source":["%%time\n","# model_all2\n","test_loss = 0\n","for j,(audio,txt) in enumerate(train_loader_WER2):\n","    #audio,input_ids,attention_mask = jnp.array(audio,dtype=dtype),jnp.array(input_ids),jnp.array(attention_mask)\n","    l = metric_one_step(audio,txt)\n","    test_loss += l\n","    if j>20:\n","        break\n","test_loss /= (j+1)\n","logging.info(f\"WER loss on data2: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
